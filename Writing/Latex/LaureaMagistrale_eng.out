\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.1.1}{Motivation and background}{chapter.1}% 2
\BOOKMARK [1][-]{section.1.2}{Theoretical framework and focus of the study}{chapter.1}% 3
\BOOKMARK [1][-]{section.1.3}{Research questions and objectives}{chapter.1}% 4
\BOOKMARK [1][-]{section.1.4}{Methodology}{chapter.1}% 5
\BOOKMARK [1][-]{section.1.5}{Structure of the thesis}{chapter.1}% 6
\BOOKMARK [0][-]{chapter.2}{Machine learning: Some theoretical concepts}{}% 7
\BOOKMARK [1][-]{section.2.1}{Data collection and preprocessing}{chapter.2}% 8
\BOOKMARK [2][-]{subsection.2.1.1}{ETL}{section.2.1}% 9
\BOOKMARK [2][-]{subsection.2.1.2}{Apache spark}{section.2.1}% 10
\BOOKMARK [2][-]{subsection.2.1.3}{Dealing with missing data}{section.2.1}% 11
\BOOKMARK [2][-]{subsection.2.1.4}{Dealing with imbalance data}{section.2.1}% 12
\BOOKMARK [2][-]{subsection.2.1.5}{One-hot encoding}{section.2.1}% 13
\BOOKMARK [2][-]{subsection.2.1.6}{Ordinal encoding}{section.2.1}% 14
\BOOKMARK [2][-]{subsection.2.1.7}{Categorical embeddings}{section.2.1}% 15
\BOOKMARK [2][-]{subsection.2.1.8}{Feature selection}{section.2.1}% 16
\BOOKMARK [1][-]{section.2.2}{Machine learning models}{chapter.2}% 17
\BOOKMARK [2][-]{subsection.2.2.1}{Logistic regression}{section.2.2}% 18
\BOOKMARK [2][-]{subsection.2.2.2}{Decision tree classifier}{section.2.2}% 19
\BOOKMARK [2][-]{subsection.2.2.3}{Random forest classifier}{section.2.2}% 20
\BOOKMARK [2][-]{subsection.2.2.4}{Boosting: Gradient-boosted tree, XGBoost}{section.2.2}% 21
\BOOKMARK [1][-]{section.2.3}{Evaluation metrics}{chapter.2}% 22
\BOOKMARK [2][-]{subsection.2.3.1}{Confusion matrix}{section.2.3}% 23
\BOOKMARK [2][-]{subsection.2.3.2}{Precision, Recall, F-Measure}{section.2.3}% 24
\BOOKMARK [2][-]{subsection.2.3.3}{Area under the curve \(ROC AUC, PR AUC\)}{section.2.3}% 25
\BOOKMARK [0][-]{chapter.3}{Related work}{}% 26
\BOOKMARK [1][-]{section.3.1}{Review: Techniques and Data}{chapter.3}% 27
\BOOKMARK [1][-]{section.3.2}{Summary}{chapter.3}% 28
\BOOKMARK [0][-]{chapter.4}{Energy provider case study: churn prediction machine learning model}{}% 29
\BOOKMARK [1][-]{section.4.1}{Tools and libraries}{chapter.4}% 30
\BOOKMARK [2][-]{subsection.4.1.1}{Python}{section.4.1}% 31
\BOOKMARK [2][-]{subsection.4.1.2}{Apache Spark}{section.4.1}% 32
\BOOKMARK [1][-]{section.4.2}{Data collection}{chapter.4}% 33
\BOOKMARK [2][-]{subsection.4.2.1}{Data collection: ETL}{section.4.2}% 34
\BOOKMARK [2][-]{subsection.4.2.2}{Feature creation}{section.4.2}% 35
\BOOKMARK [1][-]{section.4.3}{Data description and understanding}{chapter.4}% 36
\BOOKMARK [1][-]{section.4.4}{Data preprocessing and feature selection}{chapter.4}% 37
\BOOKMARK [2][-]{subsection.4.4.1}{Handling missing data}{section.4.4}% 38
\BOOKMARK [2][-]{subsection.4.4.2}{Dealing with categorical features}{section.4.4}% 39
\BOOKMARK [2][-]{subsection.4.4.3}{Train-Test split}{section.4.4}% 40
\BOOKMARK [2][-]{subsection.4.4.4}{Imbalanced data}{section.4.4}% 41
\BOOKMARK [2][-]{subsection.4.4.5}{Feature selection}{section.4.4}% 42
\BOOKMARK [2][-]{subsection.4.4.6}{The complete workflow}{section.4.4}% 43
\BOOKMARK [0][-]{chapter.5}{Models and results}{}% 44
\BOOKMARK [1][-]{section.5.1}{Logistic regression}{chapter.5}% 45
\BOOKMARK [1][-]{section.5.2}{Decision tree classifier}{chapter.5}% 46
\BOOKMARK [1][-]{section.5.3}{Random forest classifier}{chapter.5}% 47
\BOOKMARK [1][-]{section.5.4}{Gradient-boosted tree classifier}{chapter.5}% 48
\BOOKMARK [1][-]{section.5.5}{XGBoost}{chapter.5}% 49
\BOOKMARK [1][-]{section.5.6}{Summary and analysis of the results}{chapter.5}% 50
\BOOKMARK [2][-]{subsection.5.6.1}{Random search}{section.5.6}% 51
\BOOKMARK [2][-]{subsection.5.6.2}{Threshold optimization and final results}{section.5.6}% 52
\BOOKMARK [0][-]{chapter.6}{Conclusions}{}% 53
\BOOKMARK [1][-]{section.6.1}{Conclusion}{chapter.6}% 54
\BOOKMARK [1][-]{section.6.2}{Suggestion for future research}{chapter.6}% 55
\BOOKMARK [0][-]{section*.63}{Bibliography}{}% 56
